{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "class Multiclass_Logistic_Regression:\n",
    "    def __init__(self, n_in, n_out, lr):\n",
    "        self.w = np.zeros((n_out,n_in))\n",
    "        self.b = np.zeros(n_out)\n",
    "        self.learn_rate = lr\n",
    "\n",
    "    def softmax(self,x):\n",
    "        return np.exp(x)/np.sum(np.exp(x),axis=1,keepdims=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        pre_y = self.softmax(np.dot(x,self.w.T)+self.b)\n",
    "        return pre_y\n",
    "\n",
    "    def backward(self,input_x,pre_y,label_y):\n",
    "        temp = pre_y - label_y\n",
    "        delta_w = np.dot(temp.T,input_x)\n",
    "        delta_b = np.sum(temp,axis=0)/len(label_y)\n",
    "        \n",
    "        self.w -= delta_w*self.learn_rate\n",
    "        self.b -= delta_b*self.learn_rate\n",
    "\n",
    "    def cross_entropy_loss(self,pre_y,label_y):\n",
    "        cross_entropy_loss = -np.sum(label_y*np.log(pre_y))/len(label_y)\n",
    "        return cross_entropy_loss\n",
    "\n",
    "    def accuracy(self,pre_y,label_y):\n",
    "        pre_y = np.argmax(pre_y,axis=1)\n",
    "        y = np.argmax(label_y,axis=1)\n",
    "        return np.sum(pre_y==y)/len(label_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training: Epoch0, Loss: 0.43, Accuracy87.57%\n",
      "Training: Epoch1, Loss: 0.33, Accuracy90.77%\n",
      "Training: Epoch2, Loss: 0.31, Accuracy91.19%\n",
      "Training: Epoch3, Loss: 0.31, Accuracy91.29%\n",
      "Training: Epoch4, Loss: 0.30, Accuracy91.53%\n",
      "Training: Epoch5, Loss: 0.30, Accuracy91.69%\n",
      "Training: Epoch6, Loss: 0.29, Accuracy91.81%\n",
      "Training: Epoch7, Loss: 0.29, Accuracy92.01%\n",
      "Training: Epoch8, Loss: 0.29, Accuracy91.86%\n",
      "Training: Epoch9, Loss: 0.29, Accuracy91.97%\n",
      "Training: Epoch10, Loss: 0.28, Accuracy92.04%\n",
      "Training: Epoch11, Loss: 0.28, Accuracy92.12%\n",
      "Training: Epoch12, Loss: 0.28, Accuracy92.21%\n",
      "Training: Epoch13, Loss: 0.28, Accuracy92.20%\n",
      "Training: Epoch14, Loss: 0.28, Accuracy92.21%\n",
      "Training: Epoch15, Loss: 0.28, Accuracy92.23%\n",
      "Training: Epoch16, Loss: 0.28, Accuracy92.24%\n",
      "Training: Epoch17, Loss: 0.28, Accuracy92.23%\n",
      "Training: Epoch18, Loss: 0.28, Accuracy92.28%\n",
      "Training: Epoch19, Loss: 0.27, Accuracy92.33%\n",
      "Training: Epoch20, Loss: 0.28, Accuracy92.31%\n",
      "Training: Epoch21, Loss: 0.27, Accuracy92.45%\n",
      "Training: Epoch22, Loss: 0.27, Accuracy92.39%\n",
      "Training: Epoch23, Loss: 0.27, Accuracy92.32%\n",
      "Training: Epoch24, Loss: 0.27, Accuracy92.43%\n",
      "Training: Epoch25, Loss: 0.27, Accuracy92.34%\n",
      "Training: Epoch26, Loss: 0.27, Accuracy92.45%\n",
      "Training: Epoch27, Loss: 0.27, Accuracy92.45%\n",
      "Training: Epoch28, Loss: 0.27, Accuracy92.39%\n",
      "Training: Epoch29, Loss: 0.27, Accuracy92.35%\n",
      "Training: Epoch30, Loss: 0.27, Accuracy92.45%\n",
      "Training: Epoch31, Loss: 0.27, Accuracy92.51%\n",
      "Training: Epoch32, Loss: 0.27, Accuracy92.60%\n",
      "Training: Epoch33, Loss: 0.27, Accuracy92.38%\n",
      "Training: Epoch34, Loss: 0.27, Accuracy92.53%\n",
      "Training: Epoch35, Loss: 0.26, Accuracy92.67%\n",
      "Training: Epoch36, Loss: 0.26, Accuracy92.62%\n",
      "Training: Epoch37, Loss: 0.26, Accuracy92.51%\n",
      "Training: Epoch38, Loss: 0.26, Accuracy92.54%\n",
      "Training: Epoch39, Loss: 0.26, Accuracy92.64%\n",
      "Training: Epoch40, Loss: 0.26, Accuracy92.52%\n",
      "Training: Epoch41, Loss: 0.26, Accuracy92.67%\n",
      "Training: Epoch42, Loss: 0.26, Accuracy92.57%\n",
      "Training: Epoch43, Loss: 0.26, Accuracy92.63%\n",
      "Training: Epoch44, Loss: 0.26, Accuracy92.57%\n",
      "Training: Epoch45, Loss: 0.26, Accuracy92.66%\n",
      "Training: Epoch46, Loss: 0.26, Accuracy92.49%\n",
      "Training: Epoch47, Loss: 0.26, Accuracy92.59%\n",
      "Training: Epoch48, Loss: 0.26, Accuracy92.70%\n",
      "Training: Epoch49, Loss: 0.26, Accuracy92.59%\n",
      "Training: Epoch50, Loss: 0.26, Accuracy92.63%\n",
      "Training: Epoch51, Loss: 0.26, Accuracy92.78%\n",
      "Training: Epoch52, Loss: 0.26, Accuracy92.74%\n",
      "Training: Epoch53, Loss: 0.26, Accuracy92.63%\n",
      "Training: Epoch54, Loss: 0.26, Accuracy92.67%\n",
      "Training: Epoch55, Loss: 0.26, Accuracy92.67%\n",
      "Training: Epoch56, Loss: 0.26, Accuracy92.62%\n",
      "Training: Epoch57, Loss: 0.26, Accuracy92.67%\n",
      "Training: Epoch58, Loss: 0.26, Accuracy92.78%\n",
      "Training: Epoch59, Loss: 0.26, Accuracy92.73%\n",
      "Training: Epoch60, Loss: 0.26, Accuracy92.79%\n",
      "Training: Epoch61, Loss: 0.26, Accuracy92.70%\n",
      "Training: Epoch62, Loss: 0.26, Accuracy92.81%\n",
      "Training: Epoch63, Loss: 0.26, Accuracy92.68%\n",
      "Training: Epoch64, Loss: 0.26, Accuracy92.72%\n",
      "Training: Epoch65, Loss: 0.26, Accuracy92.67%\n",
      "Training: Epoch66, Loss: 0.26, Accuracy92.68%\n",
      "Training: Epoch67, Loss: 0.26, Accuracy92.77%\n",
      "Training: Epoch68, Loss: 0.26, Accuracy92.66%\n",
      "Training: Epoch69, Loss: 0.26, Accuracy92.76%\n",
      "Training: Epoch70, Loss: 0.26, Accuracy92.73%\n",
      "Training: Epoch71, Loss: 0.26, Accuracy92.79%\n",
      "Training: Epoch72, Loss: 0.26, Accuracy92.86%\n",
      "Training: Epoch73, Loss: 0.26, Accuracy92.68%\n",
      "Training: Epoch74, Loss: 0.25, Accuracy92.89%\n",
      "Training: Epoch75, Loss: 0.26, Accuracy92.81%\n",
      "Training: Epoch76, Loss: 0.26, Accuracy92.77%\n",
      "Training: Epoch77, Loss: 0.26, Accuracy92.73%\n",
      "Training: Epoch78, Loss: 0.25, Accuracy92.94%\n",
      "Training: Epoch79, Loss: 0.25, Accuracy92.81%\n",
      "Training: Epoch80, Loss: 0.25, Accuracy92.92%\n",
      "Training: Epoch81, Loss: 0.25, Accuracy92.85%\n",
      "Training: Epoch82, Loss: 0.26, Accuracy92.86%\n",
      "Training: Epoch83, Loss: 0.25, Accuracy92.81%\n",
      "Training: Epoch84, Loss: 0.25, Accuracy92.86%\n",
      "Training: Epoch85, Loss: 0.25, Accuracy92.87%\n",
      "Training: Epoch86, Loss: 0.25, Accuracy92.75%\n",
      "Training: Epoch87, Loss: 0.25, Accuracy92.88%\n",
      "Training: Epoch88, Loss: 0.25, Accuracy92.78%\n",
      "Training: Epoch89, Loss: 0.25, Accuracy92.95%\n",
      "Training: Epoch90, Loss: 0.25, Accuracy92.83%\n",
      "Training: Epoch91, Loss: 0.25, Accuracy92.91%\n",
      "Training: Epoch92, Loss: 0.25, Accuracy92.94%\n",
      "Training: Epoch93, Loss: 0.25, Accuracy92.88%\n",
      "Training: Epoch94, Loss: 0.25, Accuracy92.91%\n",
      "Training: Epoch95, Loss: 0.25, Accuracy92.87%\n",
      "Training: Epoch96, Loss: 0.25, Accuracy92.95%\n",
      "Training: Epoch97, Loss: 0.25, Accuracy92.77%\n",
      "Training: Epoch98, Loss: 0.25, Accuracy92.96%\n",
      "Training: Epoch99, Loss: 0.25, Accuracy92.86%\n",
      "Testing\n",
      "Test:Loss0.30, Test Accuracy91.98%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "batch_size = 100\n",
    "epoch = 50\n",
    "n_in =  784\n",
    "n_out = 10\n",
    "learn_rate = 0.01\n",
    "\n",
    "batch_num = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "#train\n",
    "model = Multiclass_Logistic_Regression(n_in,n_out,learn_rate)\n",
    "for ep in range(epoch):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for i in range(batch_num): \n",
    "        input_x,label_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        y = model.forward(input_x)\n",
    "        accuracy += model.accuracy(y,label_y)\n",
    "        loss += model.cross_entropy_loss(y,label_y)\n",
    "        \n",
    "        model.backward(input_x,y,label_y)\n",
    "        \n",
    "        \n",
    "    print('Training: Epoch{0}, Loss: {2:.2f}, Accuracy{3:.2f}%'.format(ep,mnist.train.num_examples,loss/batch_num,accuracy/batch_num*100))\n",
    " \n",
    "#testbatch_num\n",
    "y = model.forward(mnist.validation.images)\n",
    "accuracy = model.accuracy(y,mnist.validation.labels)\n",
    "loss = model.cross_entropy_loss(y,mnist.validation.labels)\n",
    "print('Testing')\n",
    "print('Test:Loss{1:.2f}, Test Accuracy{2:.2f}%'.format(mnist.validation.num_examples,loss,accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.8_gpu]",
   "language": "python",
   "name": "conda-env-tensorflow1.8_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self, _input, _input_num, _classes_num):\n",
    "        # weight\n",
    "        self.W = tf.Variable(tf.zeros([_input_num, _classes_num]), dtype=tf.float32)\n",
    "        # bias\n",
    "        self.b = tf.Variable(tf.zeros([_classes_num,]), dtype=tf.float32)\n",
    "        # output\n",
    "        self.output = tf.nn.softmax(tf.matmul(_input, self.W) + self.b)\n",
    "        # prediction\n",
    "        self.y_pred = tf.argmax(self.output, axis=1)\n",
    "\n",
    "    \n",
    "class HiddenLayer:\n",
    "    def __init__(self, _input, _input_num, _output_num, activ_func = tf.nn.sigmoid):\n",
    "        # weight\n",
    "        bound_val = 4.0*np.sqrt(6.0/(_input_num + _output_num))\n",
    "        self.W = tf.Variable(tf.random_uniform([_input_num, _output_num], minval=-bound_val, maxval=bound_val),dtype=tf.float32, name=\"W\")\n",
    "        # bias\n",
    "        self.b = tf.Variable(tf.zeros([_output_num,]), dtype=tf.float32, name=\"b\")\n",
    "        \n",
    "        # output    print(mnist.test.images.shape)\n",
    "        if activ_func is None:\n",
    "            self.output = tf.matmul(_input, self.W) + self.b\n",
    "        else:\n",
    "            self.output = activ_func(tf.matmul(_input, self.W) + self.b)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, _input, _input_num, _hidden_num, _output_num):\n",
    "        # Set up the model\n",
    "        last_output = _input\n",
    "        last_output_num = _input_num\n",
    "        \n",
    "        self.hiddenlayer = []\n",
    "        for i in range(len(_hidden_num)):\n",
    "            self.hiddenlayer.append( HiddenLayer(last_output, _input_num = last_output_num, _output_num =_hidden_num[i]) )\n",
    "            last_output = self.hiddenlayer[i].output\n",
    "            last_output_num = _hidden_num[i]\n",
    "\n",
    "        \n",
    "        self.outputlayer = SoftmaxLayer(last_output, _input_num=last_output_num, _classes_num=_output_num)\n",
    "        \n",
    "        # prediction\n",
    "        self.y_pred = self.outputlayer.y_pred\n",
    "\n",
    "\n",
    "    def cross_entropy_loss(self, y):\n",
    "        return -tf.reduce_mean(tf.reduce_sum(y * tf.log(self.outputlayer.output), axis=1))\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        correct_pred = tf.equal(self.outputlayer.y_pred, tf.argmax(y, axis=1))\n",
    "        return tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training...\n",
      "Epoch 0: loss: 0.9155231214653358, validation accuacy: 0.8687999844551086\n",
      "Epoch 10: loss: 0.33936947516419635, validation accuacy: 0.9121999740600586\n",
      "Epoch 20: loss: 0.30947883848439556, validation accuacy: 0.9214000105857849\n",
      "Epoch 30: loss: 0.29555775191296224, validation accuacy: 0.9236000180244446\n",
      "Epoch 40: loss: 0.2870824030854482, validation accuacy: 0.9243999719619751\n",
      "Epoch 50: loss: 0.2810914056951349, validation accuacy: 0.925000011920929\n",
      "Epoch 60: loss: 0.2765442376651547, validation accuacy: 0.9265999794006348\n",
      "Epoch 70: loss: 0.2729456642270089, validation accuacy: 0.926800012588501\n",
      "Epoch 80: loss: 0.2700091463869269, validation accuacy: 0.9273999929428101\n",
      "Epoch 90: loss: 0.26747991110790864, validation accuacy: 0.9269999861717224\n",
      "Epoch 100: loss: 0.2654429981518875, validation accuacy: 0.9282000064849854\n",
      "Epoch 110: loss: 0.2635142041607338, validation accuacy: 0.9276000261306763\n",
      "Epoch 120: loss: 0.26177584801207887, validation accuacy: 0.9273999929428101\n",
      "Epoch 130: loss: 0.26031153080138303, validation accuacy: 0.9269999861717224\n",
      "Epoch 140: loss: 0.25897031304511126, validation accuacy: 0.9276000261306763\n",
      "Epoch 150: loss: 0.2577062284269116, validation accuacy: 0.9291999936103821\n",
      "Epoch 160: loss: 0.256572834253311, validation accuacy: 0.9279999732971191\n",
      "Epoch 170: loss: 0.2555775100399148, validation accuacy: 0.928600013256073\n",
      "Epoch 180: loss: 0.2545193563537164, validation accuacy: 0.9279999732971191\n",
      "Epoch 190: loss: 0.25359279845248583, validation accuacy: 0.9287999868392944\n",
      "Testing...\n",
      "Test: accuacy: 0.9244999885559082\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load mnist dataset\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    \n",
    "    # define training param\n",
    "    epochs_num = 200\n",
    "    batch_size = 100\n",
    "    display_step = 10\n",
    "    batch_num = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    \n",
    "    # define input and output placehoders\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    # create mlp model\n",
    "    multiclass_logistic_regressor = MLP(_input=x, _input_num=784, _hidden_num=[], _output_num=10)\n",
    "    # get loss\n",
    "    loss = multiclass_logistic_regressor.cross_entropy_loss(y_)\n",
    "    # accuracy\n",
    "    accuracy = multiclass_logistic_regressor.accuracy(y_)\n",
    "    predictor = multiclass_logistic_regressor.y_pred\n",
    "    \n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.02).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs_num):\n",
    "            avg_loss = 0.0\n",
    "            \n",
    "            for i in range(batch_num):\n",
    "                x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run(train_op, feed_dict={x: x_batch, y_: y_batch})\n",
    "                avg_loss += sess.run(loss, feed_dict={x: x_batch, y_: y_batch}) / batch_num\n",
    "\n",
    "            if epoch % display_step == 0:\n",
    "                val_acc = sess.run(accuracy, feed_dict={x: mnist.validation.images,\n",
    "                                                       y_: mnist.validation.labels})\n",
    "                print(\"Epoch {0}: loss: {1}, validation accuacy: {2}\".format(epoch,\n",
    "                                                                            avg_loss, val_acc))\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images,y_: mnist.test.labels})\n",
    "        print(\"Test: accuacy: {0}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) MLP with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 0: loss: 1.22101874990897, validation accuacy: 0.8416000008583069\n",
      "Epoch 10: loss: 0.33905528894879605, validation accuacy: 0.9124000072479248\n",
      "Epoch 20: loss: 0.2913135877116159, validation accuacy: 0.9246000051498413\n",
      "Epoch 30: loss: 0.26488797327334224, validation accuacy: 0.9318000078201294\n",
      "Epoch 40: loss: 0.24438670064915322, validation accuacy: 0.9354000091552734\n",
      "Epoch 50: loss: 0.22694157447327262, validation accuacy: 0.9387999773025513\n",
      "Epoch 60: loss: 0.21118372019041678, validation accuacy: 0.9431999921798706\n",
      "Epoch 70: loss: 0.19704378839243544, validation accuacy: 0.9473999738693237\n",
      "Epoch 80: loss: 0.1840471007133073, validation accuacy: 0.949400007724762\n",
      "Epoch 90: loss: 0.17254632462154784, validation accuacy: 0.9527999758720398\n",
      "Epoch 100: loss: 0.1618204302611676, validation accuacy: 0.9544000029563904\n",
      "Epoch 110: loss: 0.15227055092426875, validation accuacy: 0.9567999839782715\n",
      "Epoch 120: loss: 0.14353096114640884, validation accuacy: 0.9584000110626221\n",
      "Epoch 130: loss: 0.13562946048649877, validation accuacy: 0.9603999853134155\n",
      "Epoch 140: loss: 0.1285099043527787, validation accuacy: 0.9620000123977661\n",
      "Epoch 150: loss: 0.12188729897818766, validation accuacy: 0.9635999798774719\n",
      "Epoch 160: loss: 0.11584684016013683, validation accuacy: 0.965399980545044\n",
      "Epoch 170: loss: 0.1104012945497578, validation accuacy: 0.967199981212616\n",
      "Epoch 180: loss: 0.10525592881170169, validation accuacy: 0.9684000015258789\n",
      "Epoch 190: loss: 0.10052603564817793, validation accuacy: 0.968999981880188\n",
      "Testing...\n",
      "Test: accuacy: 0.9656999707221985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create mlp model\n",
    "mlp_classifier1 = MLP(_input=x, _input_num=784, _hidden_num=[500], _output_num=10)\n",
    "# get loss\n",
    "loss = mlp_classifier1.cross_entropy_loss(y_) \n",
    "# accuracy\n",
    "accuracy = mlp_classifier1.accuracy(y_)\n",
    "predictor = mlp_classifier1.y_pred\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.02).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "print(\"Training...\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs_num):\n",
    "        avg_loss = 0.0\n",
    "            \n",
    "        for i in range(batch_num):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_op, feed_dict={x: x_batch, y_: y_batch})\n",
    "            avg_loss += sess.run(loss, feed_dict={x: x_batch, y_: y_batch}) / batch_num\n",
    "                \n",
    "        if epoch % display_step == 0:\n",
    "            val_acc = sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})\n",
    "            print(\"Epoch {0}: loss: {1}, validation accuacy: {2}\".format(epoch, avg_loss, val_acc))\n",
    "\n",
    "    print(\"Testing...\")\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images,y_: mnist.test.labels})\n",
    "    print(\"Test: accuacy: {0}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) MLP with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 0: loss: 1.7305880247462884, validation accuacy: 0.7505999803543091\n",
      "Epoch 10: loss: 0.3323439566113735, validation accuacy: 0.9120000004768372\n",
      "Epoch 20: loss: 0.2670271993496201, validation accuacy: 0.9265999794006348\n",
      "Epoch 30: loss: 0.23142414401878017, validation accuacy: 0.9368000030517578\n",
      "Epoch 40: loss: 0.2049292043393308, validation accuacy: 0.9426000118255615\n",
      "Epoch 50: loss: 0.18306523636660793, validation accuacy: 0.9462000131607056\n",
      "Epoch 60: loss: 0.16505721724168823, validation accuacy: 0.9513999819755554\n",
      "Epoch 70: loss: 0.1499371575767343, validation accuacy: 0.9567999839782715\n",
      "Epoch 80: loss: 0.1372461358728732, validation accuacy: 0.9593999981880188\n",
      "Epoch 90: loss: 0.12612536076794978, validation accuacy: 0.9617999792098999\n",
      "Epoch 100: loss: 0.11640308991413233, validation accuacy: 0.9631999731063843\n",
      "Epoch 110: loss: 0.1080524612218141, validation accuacy: 0.9649999737739563\n",
      "Epoch 120: loss: 0.10062915821644391, validation accuacy: 0.9664000272750854\n",
      "Epoch 130: loss: 0.09388303806497297, validation accuacy: 0.9678000211715698\n",
      "Epoch 140: loss: 0.08788438382473854, validation accuacy: 0.9696000218391418\n",
      "Epoch 150: loss: 0.08235694696280084, validation accuacy: 0.9696000218391418\n",
      "Epoch 160: loss: 0.07734884374859659, validation accuacy: 0.97079998254776\n",
      "Epoch 170: loss: 0.07270646106113085, validation accuacy: 0.972000002861023\n",
      "Epoch 180: loss: 0.06849353176964966, validation accuacy: 0.9733999967575073\n",
      "Epoch 190: loss: 0.06463070340285244, validation accuacy: 0.9747999906539917\n",
      "Testing...\n",
      "Test: accuacy: 0.972100019454956\n"
     ]
    }
   ],
   "source": [
    "# create mlp model\n",
    "mlp_classifier1 = MLP(_input=x, _input_num=784, _hidden_num=[500, 250], _output_num=10)\n",
    "# get loss\n",
    "loss = mlp_classifier1.cross_entropy_loss(y_) \n",
    "# accuracy\n",
    "accuracy = mlp_classifier1.accuracy(y_)\n",
    "predictor = mlp_classifier1.y_pred\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.02).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "print(\"Training...\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs_num):\n",
    "        avg_loss = 0.0\n",
    "            \n",
    "        for i in range(batch_num):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_op, feed_dict={x: x_batch, y_: y_batch})\n",
    "            avg_loss += sess.run(loss, feed_dict={x: x_batch, y_: y_batch}) / batch_num\n",
    "                \n",
    "        if epoch % display_step == 0:\n",
    "            val_acc = sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})\n",
    "            print(\"Epoch {0}: loss: {1}, validation accuacy: {2}\".format(epoch, avg_loss, val_acc))\n",
    "\n",
    "    print(\"Testing...\")\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images,y_: mnist.test.labels})\n",
    "    print(\"Test: accuacy: {0}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.8_gpu]",
   "language": "python",
   "name": "conda-env-tensorflow1.8_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
